{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 07:18:41 [__init__.py:256] Automatically detected platform cuda.\n",
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "print(vllm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 07:18:49 [config.py:583] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-24 07:18:49 [arg_utils.py:1776] LORA is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\n",
      "INFO 03-24 07:18:49 [config.py:1515] Defaulting to use mp for distributed inference\n",
      "WARNING 03-24 07:18:49 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 03-24 07:18:49 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=5120, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "WARNING 03-24 07:18:49 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:18:49 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "INFO 03-24 07:18:50 [cuda.py:285] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:18:50 [cuda.py:285] Using Flash Attention backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W324 07:19:01.956069649 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W324 07:19:01.079424383 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W324 07:19:11.966748141 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 07:19:21 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "INFO 03-24 07:19:21 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:19:21 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:19:21 [pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W324 07:19:21.976069680 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 07:19:21 [custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 03-24 07:19:31 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:19:31 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 03-24 07:19:31 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_df7f7be7'), local_subscribe_addr='ipc:///tmp/3c9b5334-9213-4d63-ae47-bb7ff7104ba6', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 03-24 07:19:31 [parallel_state.py:967] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:19:31 [parallel_state.py:967] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 03-24 07:19:31 [model_runner.py:1110] Starting to load model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:19:31 [model_runner.py:1110] Starting to load model /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b356824014a643b1833e736e9ae6e0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:19:33 [loader.py:429] Loading weights took 1.97 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:19:33 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-24 07:19:33 [loader.py:429] Loading weights took 2.02 seconds\n",
      "INFO 03-24 07:19:33 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:19:33 [model_runner.py:1146] Model loading took 7.1700 GB and 2.176253 seconds\n",
      "INFO 03-24 07:19:33 [model_runner.py:1146] Model loading took 7.1700 GB and 2.114723 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:19:40 [worker.py:267] Memory profiling takes 6.04 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:19:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (23.49GiB) x gpu_memory_utilization (0.90) = 21.14GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1081324)\u001b[0;0m INFO 03-24 07:19:40 [worker.py:267] model weights take 7.17GiB; non_torch_memory takes 0.31GiB; PyTorch activation peak memory takes 0.45GiB; the rest of the memory reserved for KV Cache is 13.20GiB.\n",
      "INFO 03-24 07:19:40 [worker.py:267] Memory profiling takes 6.10 seconds\n",
      "INFO 03-24 07:19:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (23.49GiB) x gpu_memory_utilization (0.90) = 21.14GiB\n",
      "INFO 03-24 07:19:40 [worker.py:267] model weights take 7.17GiB; non_torch_memory takes 0.33GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 12.21GiB.\n",
      "INFO 03-24 07:19:40 [executor_base.py:111] # cuda blocks: 28588, # CPU blocks: 9362\n",
      "INFO 03-24 07:19:40 [executor_base.py:116] Maximum concurrency for 5120 tokens per request: 89.34x\n",
      "INFO 03-24 07:19:42 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 9.04 seconds\n"
     ]
    }
   ],
   "source": [
    "#本地模型目录\n",
    "cache_dir = '/root/.cache/modelscope/hub/models'\n",
    "model_name = f'{cache_dir}/Qwen/Qwen2.5-7B-Instruct'\n",
    "\n",
    "# Lora\n",
    "lora_path='/data/healthai/checkpoints/checkpoint-279'\n",
    "\n",
    "from vllm.lora.request import LoRARequest\n",
    "llm = vllm.LLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    trust_remote_code=True,\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    enable_lora=True\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n\\n    你是一位经验丰富的临床医学专家，对各种疾病的症状、体征、检查结果和诊断标准有着深入的了解。同时，你擅长从病历中提取关键信息，运用专业知识进行综合分析，为疾病的诊断提供准确的依据。\\n    <|im_end|>\\n<|im_start|>user\\n性别: 女\\n年龄: 50\\n主诉: 发现血糖升高3月\\n现病史: 发现血糖升高3月，目前服用二甲双胍治疗，空腹血糖仍有13mmol\\\\/l左右.\\n既往史: 无特殊\\n个人史: \\n过敏史: 无\\n婚育史: \\n流行病史: □1、发病前14天内有病例报告社区的旅行史或居住史；□2、发病前14天内与新型冠状病毒感染的患者或无症状感染者有接触史；□3、发病前14天内曾接触过来自有病例报告社区的发热或有呼吸道症状的患者；□4、聚集性发病（2周内在小范围如家庭、办公室、学校班级等场所，出现2例及以上发热和\\\\/或呼吸道症状的病例）；√5、无上述流行病学史。\\n体格检查: 血压（收缩压\\\\/舒张压）120\\\\/60mmHg  \\n  空腹血糖测定值：mmol\\\\/L  \\n  餐后血糖测定值：mmol\\\\/L  \\n  T：℃  \\n  P：次\\\\/分  \\n  R：次\\\\/分  \\n  神志：   \\n  呼吸：  \\n辅助检查: 暂无<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_chat_template():\n",
    "    system_prompt = \"\"\"\n",
    "    你是一位经验丰富的临床医学专家，对各种疾病的症状、体征、检查结果和诊断标准有着深入的了解。同时，你擅长从病历中提取关键信息，运用专业知识进行综合分析，为疾病的诊断提供准确的依据。\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{system_prompt}\"},\n",
    "        {\"role\": \"user\", \"content\": \"性别: 女\\n年龄: 50\\n主诉: 发现血糖升高3月\\n现病史: 发现血糖升高3月，目前服用二甲双胍治疗，空腹血糖仍有13mmol\\/l左右.\\n既往史: 无特殊\\n个人史: \\n过敏史: 无\\n婚育史: \\n流行病史: □1、发病前14天内有病例报告社区的旅行史或居住史；□2、发病前14天内与新型冠状病毒感染的患者或无症状感染者有接触史；□3、发病前14天内曾接触过来自有病例报告社区的发热或有呼吸道症状的患者；□4、聚集性发病（2周内在小范围如家庭、办公室、学校班级等场所，出现2例及以上发热和\\/或呼吸道症状的病例）；√5、无上述流行病学史。\\n体格检查: 血压（收缩压\\/舒张压）120\\/60mmHg  \\n  空腹血糖测定值：mmol\\/L  \\n  餐后血糖测定值：mmol\\/L  \\n  T：℃  \\n  P：次\\/分  \\n  R：次\\/分  \\n  神志：   \\n  呼吸：  \\n辅助检查: 暂无\"}\n",
    "    ]\n",
    "\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "apply_chat_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.91s/it, est. speed input: 27.51 toks/s, output: 25.26 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionOutput(index=0, text='根据患者的病历信息，我们进行如下分析：\\n\\n1. 患者主诉发现血糖升高3个月，目前为空腹血糖13mmol/L，远高于正常范围，提示存在明显的高血糖状态。\\n2. 患者接受二甲双胍治疗，但空腹血糖仍高达13mmol/L，提示可能存在药物抵抗或病情未能得到有效控制。\\n3. 体格检查显示血压正常（120/60mmHg），但未提供具体的空腹血糖和餐后血糖值，无法进一步评估糖代谢状态。\\n4. 患者既往史、个人史、过敏史均无特殊记录，婚育史及流行病学史无异常。\\n5. 需要进行进一步的检查，如糖化血红蛋白（HbA1c）、肝肾功能、胰岛素水平等，以评估糖尿病的控制情况及是否存在其他代谢异常。\\n6. 糖化血红蛋白（HbA1c）是一种评估血糖控制情况的重要指标，其值通常用于跟踪患者长期的血糖变化。\\n7. 二甲双胍是常用的降糖药物，适用于2型糖尿病患者的治疗，但可能对某些患者效果欠佳，需进一步评估是否需要调整药物或联合治疗。\\n8. 可能存在其他代谢异常或糖尿病相关并发症，建议进行进一步检查以排除其他疾病可能性。\\n\\n结论：患者需进一步评估血糖控制情况，明确是否存在糖尿病病史及其代谢异常，并根据检查结果调整治疗方案。', token_ids=(100345, 108723, 99252, 81202, 27369, 3837, 97639, 71817, 104506, 101042, 48443, 16, 13, 6567, 224, 96, 28946, 35568, 99482, 99879, 110162, 109061, 18, 99943, 3837, 100004, 50647, 101317, 110162, 16, 18, 3821, 337, 7434, 3837, 99427, 105688, 100416, 101121, 3837, 45139, 47606, 106050, 44636, 110162, 44091, 8997, 17, 13, 6567, 224, 96, 28946, 100669, 40820, 100043, 99493, 120180, 101899, 3837, 77288, 34794, 101317, 110162, 99754, 104930, 16, 18, 3821, 337, 7434, 3837, 45139, 116116, 104459, 103439, 57191, 106141, 106197, 118130, 100359, 8997, 18, 13, 220, 31914, 33983, 101071, 54021, 103285, 100416, 9909, 16, 17, 15, 14, 21, 15, 3821, 39, 70, 48272, 77288, 38342, 99553, 108247, 34794, 101317, 110162, 33108, 99512, 33447, 110162, 25511, 3837, 101068, 100642, 102086, 100443, 103225, 44091, 8997, 19, 13, 6567, 224, 96, 28946, 99929, 99321, 99497, 5373, 99605, 99497, 5373, 107270, 99497, 99472, 42192, 101167, 65577, 3837, 99838, 99241, 99497, 81217, 102529, 99252, 47764, 99497, 42192, 70633, 8997, 20, 13, 18137, 250, 222, 30534, 71817, 100642, 9370, 101071, 3837, 29524, 100443, 32108, 99389, 99425, 102835, 9909, 39, 65, 32, 16, 66, 64359, 101364, 102512, 98380, 5373, 119386, 99825, 71138, 100021, 49567, 3837, 23031, 102086, 107218, 9370, 100359, 99559, 81217, 116307, 92894, 103225, 70633, 8997, 21, 13, 10236, 111, 244, 32108, 99389, 99425, 102835, 9909, 39, 65, 32, 16, 66, 7552, 101158, 102086, 110162, 100359, 99559, 101945, 104118, 3837, 41146, 25511, 102119, 100751, 105946, 101924, 101930, 9370, 110162, 100682, 8997, 22, 13, 220, 40820, 100043, 99493, 120180, 20412, 110491, 99457, 100443, 104459, 3837, 106584, 17, 24300, 107218, 108723, 101899, 3837, 77288, 87267, 32664, 104760, 101924, 101062, 101463, 100191, 3837, 58362, 100642, 102086, 64471, 85106, 101921, 104459, 57191, 101101, 101899, 8997, 23, 13, 26853, 107, 26232, 47606, 92894, 103225, 70633, 57191, 107218, 78556, 115669, 3837, 101898, 71817, 100642, 101071, 23031, 102945, 92894, 101160, 103026, 3407, 106073, 5122, 101924, 58362, 100642, 102086, 110162, 100359, 99559, 3837, 100692, 116307, 107218, 99252, 99497, 104204, 103225, 70633, 90395, 100345, 101071, 59151, 47872, 101327, 99527, 100389, 1773, 151645), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)\n",
      "根据患者的病历信息，我们进行如下分析：\n",
      "\n",
      "1. 患者主诉发现血糖升高3个月，目前为空腹血糖13mmol/L，远高于正常范围，提示存在明显的高血糖状态。\n",
      "2. 患者接受二甲双胍治疗，但空腹血糖仍高达13mmol/L，提示可能存在药物抵抗或病情未能得到有效控制。\n",
      "3. 体格检查显示血压正常（120/60mmHg），但未提供具体的空腹血糖和餐后血糖值，无法进一步评估糖代谢状态。\n",
      "4. 患者既往史、个人史、过敏史均无特殊记录，婚育史及流行病学史无异常。\n",
      "5. 需要进行进一步的检查，如糖化血红蛋白（HbA1c）、肝肾功能、胰岛素水平等，以评估糖尿病的控制情况及是否存在其他代谢异常。\n",
      "6. 糖化血红蛋白（HbA1c）是一种评估血糖控制情况的重要指标，其值通常用于跟踪患者长期的血糖变化。\n",
      "7. 二甲双胍是常用的降糖药物，适用于2型糖尿病患者的治疗，但可能对某些患者效果欠佳，需进一步评估是否需要调整药物或联合治疗。\n",
      "8. 可能存在其他代谢异常或糖尿病相关并发症，建议进行进一步检查以排除其他疾病可能性。\n",
      "\n",
      "结论：患者需进一步评估血糖控制情况，明确是否存在糖尿病病史及其代谢异常，并根据检查结果调整治疗方案。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm.generate(\n",
    "    apply_chat_template(),\n",
    "    vllm.SamplingParams(\n",
    "        n=1,\n",
    "        skip_special_tokens=False,\n",
    "        max_tokens=5120\n",
    "    ),\n",
    "    use_tqdm=True,\n",
    "    lora_request=LoRARequest(lora_name=\"lora1\", lora_int_id=1, lora_path=lora_path)\n",
    ")\n",
    "\n",
    "for x in response:\n",
    "    print(x.outputs[0])\n",
    "    print(x.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
